# dataset_preprocessor.py
# -*- coding: utf-8 -*-
"""
æ•°æ®é›†å®Œæ•´é¢„å¤„ç†å·¥å…·
è§£å†³é—®é¢˜ï¼š
1. å›¾ç‰‡-æ ‡ç­¾ä¸åŒ¹é…ï¼ˆæ¸…ç†å­¤ç«‹æ ‡ç­¾ï¼‰
2. éªŒè¯é›†å›¾ç‰‡ç¼ºå¤±ï¼ˆé‡æ–°åˆ’åˆ†ï¼‰
3. ç±»åˆ«IDä¸ä¸€è‡´ï¼ˆé‡æ˜ å°„æˆ–æ‰©å±•é…ç½®ï¼‰
4. ç±»åˆ«ä¸å‡è¡¡ï¼ˆç»Ÿè®¡åˆ†æï¼‰
"""

import os
import shutil
import random
from pathlib import Path
from collections import defaultdict
import argparse

# ============== é…ç½®åŒº ==============

# é¡¹ç›®æ ¹ç›®å½•
PROJECT_ROOT = Path(__file__).parent.parent

# æ•°æ®é›†æ ¹ç›®å½•
DATASET_ROOT = PROJECT_ROOT / 'resources' / 'datasets'

# æºæ•°æ®è·¯å¾„
SRC_IMAGES_TRAIN = DATASET_ROOT / 'images' / 'train'
SRC_IMAGES_VAL = DATASET_ROOT / 'images' / 'val'
SRC_LABELS_TRAIN = DATASET_ROOT / 'labels' / 'train'
SRC_LABELS_VAL = DATASET_ROOT / 'labels' / 'val'

# è¾“å‡ºè·¯å¾„
OUTPUT_DIR = DATASET_ROOT / 'processed'

# éªŒè¯é›†æ¯”ä¾‹
VAL_RATIO = 0.2

# éšæœºç§å­
RANDOM_SEED = 42

# å®é™…ä½¿ç”¨çš„40ä¸ªç±»åˆ«ï¼ˆæ ¹æ®æ ‡ç­¾åˆ†æç»“æœï¼‰
FULL_CLASS_NAMES = {
    0: 'vegetable_leaves',      # èœå¶
    1: 'vegetable_roots',       # èœæ ¹
    2: 'fruit_peel',            # æœçš®
    3: 'fruit_core',            # æœæ ¸
    4: 'bone',                  # éª¨å¤´
    5: 'meat_skin',             # è‚‰çš®
    6: 'offal',                 # å†…è„
    7: 'rice',                  # ç±³é¥­
    8: 'noodles',               # é¢æ¡
    9: 'bread_crumbs',          # é¢åŒ…å±‘
    10: 'tea_leaves',           # èŒ¶å¶æ¸£
    11: 'coffee_grounds',       # å’–å•¡æ¸£
    12: 'eggshell',             # è›‹å£³
    13: 'plastic_bag',          # é£Ÿå“è¢‹
    14: 'plastic_wrap',         # ä¿é²œè†œ
    15: 'plastic_container',    # å¡‘æ–™ç›’
    16: 'paper_box',            # çº¸ç›’
    17: 'wrapping_paper',       # åŒ…è£…çº¸
    18: 'tissue',               # é¤å·¾çº¸
    19: 'zip_top_can',          # æ˜“æ‹‰ç½
    20: 'tin_can',              # ç½å¤´ç›’
    21: 'aluminum_foil',        # é“ç®”
    22: 'seasoning_bottle',     # è°ƒæ–™ç“¶
    23: 'wine_bottle',          # é…’ç“¶
    24: 'expired_seasoning',    # è¿‡æœŸè°ƒæ–™
    25: 'expired_medicine',     # è¿‡æœŸè¯å“
    26: 'cleaner_container',    # æ¸…æ´å‰‚å®¹å™¨
    27: 'battery',              # ç”µæ± 
    28: 'other_garbage',        # å…¶ä»–åƒåœ¾
    29: 'class_29',             # æ‰©å±•ç±»åˆ«
    30: 'class_30',
    31: 'class_31',
    32: 'class_32',
    33: 'class_33',
    34: 'class_34',
    35: 'class_35',
    36: 'class_36',
    37: 'class_37',
    38: 'class_38',
    39: 'class_39',
}

# ç±»åˆ«åˆ°åƒåœ¾åˆ†ç±»çš„æ˜ å°„
CLASS_CATEGORY = {
    0: 'å¨ä½™åƒåœ¾', 1: 'å¨ä½™åƒåœ¾', 2: 'å¨ä½™åƒåœ¾', 3: 'å¨ä½™åƒåœ¾',
    4: 'å¨ä½™åƒåœ¾', 5: 'å¨ä½™åƒåœ¾', 6: 'å¨ä½™åƒåœ¾', 7: 'å¨ä½™åƒåœ¾',
    8: 'å¨ä½™åƒåœ¾', 9: 'å¨ä½™åƒåœ¾', 10: 'å¨ä½™åƒåœ¾', 11: 'å¨ä½™åƒåœ¾', 12: 'å¨ä½™åƒåœ¾',
    13: 'å¯å›æ”¶ç‰©', 14: 'å¯å›æ”¶ç‰©', 15: 'å¯å›æ”¶ç‰©', 16: 'å¯å›æ”¶ç‰©',
    17: 'å¯å›æ”¶ç‰©', 18: 'å¯å›æ”¶ç‰©', 19: 'å¯å›æ”¶ç‰©', 20: 'å¯å›æ”¶ç‰©',
    21: 'å¯å›æ”¶ç‰©', 22: 'å¯å›æ”¶ç‰©', 23: 'å¯å›æ”¶ç‰©',
    24: 'æœ‰å®³åƒåœ¾', 25: 'æœ‰å®³åƒåœ¾', 26: 'æœ‰å®³åƒåœ¾', 27: 'æœ‰å®³åƒåœ¾',
    28: 'å…¶ä»–åƒåœ¾',
    29: 'å…¶ä»–åƒåœ¾', 30: 'å…¶ä»–åƒåœ¾', 31: 'å…¶ä»–åƒåœ¾', 32: 'å…¶ä»–åƒåœ¾',
    33: 'å…¶ä»–åƒåœ¾', 34: 'å…¶ä»–åƒåœ¾', 35: 'å…¶ä»–åƒåœ¾', 36: 'å…¶ä»–åƒåœ¾',
    37: 'å…¶ä»–åƒåœ¾', 38: 'å…¶ä»–åƒåœ¾', 39: 'å…¶ä»–åƒåœ¾',
}

# ============== å·¥å…·å‡½æ•° ==============

def get_image_extensions():
    return ['.jpg', '.jpeg', '.png', '.bmp', '.webp']


def find_matching_image(label_path, images_dir):
    """æŸ¥æ‰¾æ ‡ç­¾å¯¹åº”çš„å›¾ç‰‡æ–‡ä»¶"""
    stem = label_path.stem
    for ext in get_image_extensions():
        img_path = images_dir / (stem + ext)
        if img_path.exists():
            return img_path
    return None


def analyze_dataset():
    """åˆ†æå½“å‰æ•°æ®é›†çŠ¶æ€"""
    print("\n" + "=" * 60)
    print("æ­¥éª¤ 1: æ•°æ®é›†åˆ†æ")
    print("=" * 60)
    
    stats = {
        'train_images': 0,
        'train_labels': 0,
        'val_images': 0,
        'val_labels': 0,
        'train_matched': 0,
        'val_matched': 0,
        'train_orphan_labels': [],
        'val_orphan_labels': [],
        'class_counts': defaultdict(int),
        'all_classes': set(),
    }
    
    # ç»Ÿè®¡è®­ç»ƒé›†
    if SRC_IMAGES_TRAIN.exists():
        stats['train_images'] = len(list(SRC_IMAGES_TRAIN.glob('*.*')))
    if SRC_LABELS_TRAIN.exists():
        for label_file in SRC_LABELS_TRAIN.glob('*.txt'):
            stats['train_labels'] += 1
            if find_matching_image(label_file, SRC_IMAGES_TRAIN):
                stats['train_matched'] += 1
                # ç»Ÿè®¡ç±»åˆ«
                with open(label_file, 'r', encoding='utf-8') as f:
                    for line in f:
                        parts = line.strip().split()
                        if parts:
                            class_id = int(parts[0])
                            stats['class_counts'][class_id] += 1
                            stats['all_classes'].add(class_id)
            else:
                stats['train_orphan_labels'].append(label_file.name)
    
    # ç»Ÿè®¡éªŒè¯é›†
    if SRC_IMAGES_VAL.exists():
        stats['val_images'] = len(list(SRC_IMAGES_VAL.glob('*.*')))
    if SRC_LABELS_VAL.exists():
        for label_file in SRC_LABELS_VAL.glob('*.txt'):
            stats['val_labels'] += 1
            if find_matching_image(label_file, SRC_IMAGES_VAL):
                stats['val_matched'] += 1
            else:
                stats['val_orphan_labels'].append(label_file.name)
    
    # è¾“å‡ºåˆ†æç»“æœ
    print(f"\nğŸ“Š è®­ç»ƒé›†:")
    print(f"   å›¾ç‰‡: {stats['train_images']}")
    print(f"   æ ‡ç­¾: {stats['train_labels']}")
    print(f"   åŒ¹é…: {stats['train_matched']}")
    print(f"   å­¤ç«‹æ ‡ç­¾: {len(stats['train_orphan_labels'])}")
    
    print(f"\nğŸ“Š éªŒè¯é›†:")
    print(f"   å›¾ç‰‡: {stats['val_images']}")
    print(f"   æ ‡ç­¾: {stats['val_labels']}")
    print(f"   åŒ¹é…: {stats['val_matched']}")
    print(f"   å­¤ç«‹æ ‡ç­¾: {len(stats['val_orphan_labels'])}")
    
    print(f"\nğŸ“Š ç±»åˆ«ç»Ÿè®¡:")
    print(f"   å®é™…ä½¿ç”¨ç±»åˆ«æ•°: {len(stats['all_classes'])}")
    print(f"   ç±»åˆ«IDèŒƒå›´: {min(stats['all_classes'])} - {max(stats['all_classes'])}")
    
    print(f"\nğŸ“Š ç±»åˆ«åˆ†å¸ƒ (å‰10):")
    sorted_classes = sorted(stats['class_counts'].items(), key=lambda x: x[1], reverse=True)[:10]
    for class_id, count in sorted_classes:
        name = FULL_CLASS_NAMES.get(class_id, f'unknown_{class_id}')
        print(f"   {class_id:2d}: {count:>5} ({name})")
    
    return stats


def clean_orphan_labels(stats, dry_run=True):
    """æ¸…ç†å­¤ç«‹çš„æ ‡ç­¾æ–‡ä»¶"""
    print("\n" + "=" * 60)
    print("æ­¥éª¤ 2: æ¸…ç†å­¤ç«‹æ ‡ç­¾" + (" [æ¨¡æ‹Ÿè¿è¡Œ]" if dry_run else ""))
    print("=" * 60)
    
    total_removed = 0
    
    # æ¸…ç†è®­ç»ƒé›†å­¤ç«‹æ ‡ç­¾
    if stats['train_orphan_labels']:
        print(f"\nè®­ç»ƒé›†å­¤ç«‹æ ‡ç­¾: {len(stats['train_orphan_labels'])}")
        for label_name in stats['train_orphan_labels']:
            label_path = SRC_LABELS_TRAIN / label_name
            if not dry_run and label_path.exists():
                label_path.unlink()
            total_removed += 1
    
    # æ¸…ç†éªŒè¯é›†å­¤ç«‹æ ‡ç­¾
    if stats['val_orphan_labels']:
        print(f"éªŒè¯é›†å­¤ç«‹æ ‡ç­¾: {len(stats['val_orphan_labels'])}")
        for label_name in stats['val_orphan_labels']:
            label_path = SRC_LABELS_VAL / label_name
            if not dry_run and label_path.exists():
                label_path.unlink()
            total_removed += 1
    
    print(f"\n{'å°†åˆ é™¤' if dry_run else 'å·²åˆ é™¤'} {total_removed} ä¸ªå­¤ç«‹æ ‡ç­¾æ–‡ä»¶")
    return total_removed


def redistribute_dataset(dry_run=True):
    """é‡æ–°åˆ’åˆ†è®­ç»ƒé›†å’ŒéªŒè¯é›†"""
    print("\n" + "=" * 60)
    print("æ­¥éª¤ 3: é‡æ–°åˆ’åˆ†æ•°æ®é›†" + (" [æ¨¡æ‹Ÿè¿è¡Œ]" if dry_run else ""))
    print("=" * 60)
    
    # æ”¶é›†æ‰€æœ‰æœ‰æ•ˆçš„å›¾ç‰‡-æ ‡ç­¾å¯¹
    valid_pairs = []
    
    # ä»è®­ç»ƒé›†æ”¶é›†
    if SRC_LABELS_TRAIN.exists():
        for label_file in SRC_LABELS_TRAIN.glob('*.txt'):
            img_file = find_matching_image(label_file, SRC_IMAGES_TRAIN)
            if img_file:
                valid_pairs.append((img_file, label_file, 'train'))
    
    # ä»éªŒè¯é›†æ”¶é›†
    if SRC_LABELS_VAL.exists():
        for label_file in SRC_LABELS_VAL.glob('*.txt'):
            img_file = find_matching_image(label_file, SRC_IMAGES_VAL)
            if img_file:
                valid_pairs.append((img_file, label_file, 'val'))
    
    print(f"\næœ‰æ•ˆå›¾ç‰‡-æ ‡ç­¾å¯¹æ€»æ•°: {len(valid_pairs)}")
    
    # éšæœºæ‰“ä¹±å¹¶åˆ’åˆ†
    random.seed(RANDOM_SEED)
    random.shuffle(valid_pairs)
    
    val_count = int(len(valid_pairs) * VAL_RATIO)
    train_count = len(valid_pairs) - val_count
    
    train_pairs = valid_pairs[:train_count]
    val_pairs = valid_pairs[train_count:]
    
    print(f"æ–°è®­ç»ƒé›†: {len(train_pairs)} ({(1-VAL_RATIO)*100:.0f}%)")
    print(f"æ–°éªŒè¯é›†: {len(val_pairs)} ({VAL_RATIO*100:.0f}%)")
    
    if not dry_run:
        # åˆ›å»ºè¾“å‡ºç›®å½•
        out_train_images = OUTPUT_DIR / 'images' / 'train'
        out_train_labels = OUTPUT_DIR / 'labels' / 'train'
        out_val_images = OUTPUT_DIR / 'images' / 'val'
        out_val_labels = OUTPUT_DIR / 'labels' / 'val'
        
        for d in [out_train_images, out_train_labels, out_val_images, out_val_labels]:
            d.mkdir(parents=True, exist_ok=True)
        
        # å¤åˆ¶è®­ç»ƒé›†
        print("\nå¤åˆ¶è®­ç»ƒé›†...")
        for img, lbl, _ in train_pairs:
            shutil.copy2(img, out_train_images / img.name)
            shutil.copy2(lbl, out_train_labels / lbl.name)
        
        # å¤åˆ¶éªŒè¯é›†
        print("å¤åˆ¶éªŒè¯é›†...")
        for img, lbl, _ in val_pairs:
            shutil.copy2(img, out_val_images / img.name)
            shutil.copy2(lbl, out_val_labels / lbl.name)
        
        print(f"\nâœ… æ•°æ®å·²è¾“å‡ºåˆ°: {OUTPUT_DIR}")
    
    return train_count, val_count


def generate_data_yaml(num_classes=40):
    """ç”Ÿæˆdata.yamlé…ç½®æ–‡ä»¶"""
    print("\n" + "=" * 60)
    print("æ­¥éª¤ 4: ç”Ÿæˆé…ç½®æ–‡ä»¶")
    print("=" * 60)
    
    yaml_path = OUTPUT_DIR / 'data.yaml'
    
    yaml_content = f'''# å¨æˆ¿åƒåœ¾åˆ†ç±»æ•°æ®é›†é…ç½®
# è‡ªåŠ¨ç”Ÿæˆ - åŒ…å«{num_classes}ä¸ªç±»åˆ«

path: {OUTPUT_DIR.as_posix()}
train: images/train
val: images/val

nc: {num_classes}

names:
'''
    for i in range(num_classes):
        name = FULL_CLASS_NAMES.get(i, f'class_{i}')
        yaml_content += f'  {i}: {name}\n'
    
    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)
    with open(yaml_path, 'w', encoding='utf-8') as f:
        f.write(yaml_content)
    
    print(f"âœ… å·²ç”Ÿæˆ: {yaml_path}")
    return yaml_path


def generate_config_py(num_classes=40):
    """ç”ŸæˆConfig.pyé…ç½®æ–‡ä»¶"""
    config_path = OUTPUT_DIR / 'Config.py'
    
    # æ„å»ºç±»åˆ«åç§°å­—å…¸
    names = {i: FULL_CLASS_NAMES.get(i, f'class_{i}') for i in range(num_classes)}
    ch_names = list(names.values())
    
    # æ„å»ºåˆ†ç±»æŒ‡å¯¼
    classification_guide = {}
    color_map = {'å¨ä½™åƒåœ¾': 'green', 'å¯å›æ”¶ç‰©': 'blue', 'æœ‰å®³åƒåœ¾': 'red', 'å…¶ä»–åƒåœ¾': 'gray'}
    tip_map = {
        'å¨ä½™åƒåœ¾': 'è¯·æŠ•å…¥ç»¿è‰²å¨ä½™åƒåœ¾æ¡¶',
        'å¯å›æ”¶ç‰©': 'è¯·æ¸…æ´—åæŠ•å…¥è“è‰²å¯å›æ”¶åƒåœ¾æ¡¶',
        'æœ‰å®³åƒåœ¾': 'è¯·æŠ•å…¥çº¢è‰²æœ‰å®³åƒåœ¾æ¡¶',
        'å…¶ä»–åƒåœ¾': 'è¯·æŠ•å…¥ç°è‰²å…¶ä»–åƒåœ¾æ¡¶'
    }
    for i in range(num_classes):
        category = CLASS_CATEGORY.get(i, 'å…¶ä»–åƒåœ¾')
        classification_guide[i] = {
            'category': category,
            'color': color_map[category],
            'tip': tip_map[category]
        }
    
    config_content = f'''# Config.py
# -*- coding: utf-8 -*-
"""
åŸºäºYOLOv8çš„åƒåœ¾ç›®æ ‡æ£€æµ‹ç®—æ³• - é…ç½®æ–‡ä»¶
è‡ªåŠ¨ç”Ÿæˆ - {num_classes}ç±»åˆ«
"""

# å›¾ç‰‡åŠè§†é¢‘æ£€æµ‹ç»“æœä¿å­˜è·¯å¾„
save_path = 'save_data'

# ä½¿ç”¨çš„æ¨¡å‹è·¯å¾„
model_path = 'models/best.pt'

# ç±»åˆ«æ•°é‡
NUM_CLASSES = {num_classes}

# ç±»åˆ«é…ç½®
names = {repr(names)}

# ä¸­æ–‡ç±»åˆ«åç§°
CH_names = {repr(ch_names)}

# åƒåœ¾åˆ†ç±»æŒ‡å¯¼æ˜ å°„
classification_guide = {repr(classification_guide)}
'''
    
    with open(config_path, 'w', encoding='utf-8') as f:
        f.write(config_content)
    
    print(f"âœ… å·²ç”Ÿæˆ: {config_path}")
    return config_path


def main():
    parser = argparse.ArgumentParser(description='æ•°æ®é›†é¢„å¤„ç†å·¥å…·')
    parser.add_argument('--execute', action='store_true', help='æ‰§è¡Œå®é™…æ“ä½œï¼ˆé»˜è®¤ä¸ºæ¨¡æ‹Ÿè¿è¡Œï¼‰')
    parser.add_argument('--output', type=str, default='datasets/processed', help='è¾“å‡ºç›®å½•')
    parser.add_argument('--val-ratio', type=float, default=0.2, help='éªŒè¯é›†æ¯”ä¾‹')
    args = parser.parse_args()
    
    global OUTPUT_DIR, VAL_RATIO
    OUTPUT_DIR = Path(args.output)
    VAL_RATIO = args.val_ratio
    
    dry_run = not args.execute
    
    print("=" * 60)
    print("ğŸ”§ æ•°æ®é›†é¢„å¤„ç†å·¥å…·")
    print("=" * 60)
    print(f"æ¨¡å¼: {'å®é™…æ‰§è¡Œ' if args.execute else 'æ¨¡æ‹Ÿè¿è¡Œ (æ·»åŠ  --execute æ‰§è¡Œå®é™…æ“ä½œ)'}")
    print(f"è¾“å‡ºç›®å½•: {OUTPUT_DIR}")
    print(f"éªŒè¯é›†æ¯”ä¾‹: {VAL_RATIO*100:.0f}%")
    
    # 1. åˆ†ææ•°æ®é›†
    stats = analyze_dataset()
    
    # 2. æ¸…ç†å­¤ç«‹æ ‡ç­¾
    if stats['train_orphan_labels'] or stats['val_orphan_labels']:
        clean_orphan_labels(stats, dry_run=dry_run)
    
    # 3. é‡æ–°åˆ’åˆ†æ•°æ®é›†
    train_count, val_count = redistribute_dataset(dry_run=dry_run)
    
    # 4. ç”Ÿæˆé…ç½®æ–‡ä»¶
    if not dry_run:
        num_classes = max(stats['all_classes']) + 1 if stats['all_classes'] else 40
        generate_data_yaml(num_classes)
        generate_config_py(num_classes)
    
    # è¾“å‡ºæ€»ç»“
    print("\n" + "=" * 60)
    print("ğŸ“‹ é¢„å¤„ç†æ€»ç»“")
    print("=" * 60)
    print(f"åŸå§‹è®­ç»ƒé›†: {stats['train_matched']} æœ‰æ•ˆå¯¹")
    print(f"åŸå§‹éªŒè¯é›†: {stats['val_matched']} æœ‰æ•ˆå¯¹")
    print(f"å¤„ç†åè®­ç»ƒé›†: {train_count}")
    print(f"å¤„ç†åéªŒè¯é›†: {val_count}")
    print(f"å­¤ç«‹æ ‡ç­¾: {len(stats['train_orphan_labels']) + len(stats['val_orphan_labels'])} ä¸ª")
    
    if dry_run:
        print("\nâš ï¸  è¿™æ˜¯æ¨¡æ‹Ÿè¿è¡Œï¼Œæœªæ‰§è¡Œå®é™…æ“ä½œ")
        print("è¿è¡Œä»¥ä¸‹å‘½ä»¤æ‰§è¡Œå®é™…é¢„å¤„ç†:")
        print(f"  python dataset_preprocessor.py --execute --output {OUTPUT_DIR}")
    else:
        print("\nâœ… é¢„å¤„ç†å®Œæˆ!")
        print("\nä¸‹ä¸€æ­¥æ“ä½œ:")
        print(f"1. æ£€æŸ¥è¾“å‡ºç›®å½•: {OUTPUT_DIR}")
        print(f"2. å¤åˆ¶é…ç½®æ–‡ä»¶: copy {OUTPUT_DIR}\\Config.py .\\Config.py")
        print(f"3. ä¿®æ”¹train.pyä½¿ç”¨æ–°æ•°æ®é›†: data='{OUTPUT_DIR}/data.yaml'")
        print("4. è¿è¡Œè®­ç»ƒ: python train.py")


if __name__ == '__main__':
    main()
